# 广州市税务局TPASS-k8s集群安装说明

## 规划说明

根据广州市税务局现有虚拟主机情况，规划将现有的6台虚拟主机中的三台机器作为测试环境机器，另外三台机器作为生产环境机器，用于在测试环境和生产环境中安装kubernetes应用容器化系统（以下简称k8s）方便管理和监控TPASS系统有关服务。



## 硬件规划

### 测试环境

| 主机名称 |     IP地址      | 内存大小 | 磁盘数量 | 磁盘大小sda+sdb | CPU个数 | CPU核数 | CPU线程数 |    备注     |
| :------: | :-------------: | :------: | :-----: | :-----: | :-------: | :------: | :------: | :------: |
|  tpass-k8s-master  | 86.20.19.30 |    8G    |    2    | 16G+100G |    2    |    2    |     1     | ubuntu18.04 |
| tpass-k8s-node1 | 86.20.19.31 |    8G    |    2    | 16G+100G |    2    |    2    |     1     | ubuntu18.04 |
| tpass-k8s-node2 | 86.20.19.32 |    8G    |    2    | 16G+100G |    2    |    2    |     1     | ubuntu18.04 |

### 生产环境

|     主机名称     |    IP地址    | 内存大小 | 磁盘数量 | 磁盘大小sda+sdb | CPU个数 | CPU核数 | CPU线程数 |    备注     |
| :--------------: | :----------: | :------: | :------: | :-------------: | :-----: | :-----: | :-------: | :---------: |
| tpass-k8s-master | 86.20.17.148 |    8G    |    2     |    16G+100G     |    2    |    2    |     1     | ubuntu18.04 |
| tpass-k8s-node1  | 86.20.17.149 |    8G    |    2     |    16G+100G     |    2    |    2    |     1     | ubuntu18.04 |
| tpass-k8s-node2  | 86.20.17.150 |    8G    |    2     |    16G+100G     |    2    |    2    |     1     | ubuntu18.04 |



## 软件规划（6台机器操作）

- 不使用默认的user用户进行管理

- home和docker及nfs-data1文件夹分别挂载在sdb磁盘的三个分区上

- 使用ansible软件控制其他电脑机器

  

  解释：

  home为存放用户文件的/home目录挂载文件夹（/home）预设sdb磁盘内大小1G

  docker为安装docker软件后挂载的文件夹（/var/lib/docker）预设sdb磁盘内大小20G

  nfs为EFK日志索引系统挂载的文件夹（/mnt/nfs-data1）预设sdb磁盘内大小60G

  


### 解决规划1不使用默认的user用户进行管理

- 修改主机名方便区分是哪台机器（请根据实际情况设定，重新登录当前用户就能刷新出来）

```shell
hostnamectl set-hostname tpass-k8s-master1
# 三台机器命名：tpass-k8s-master1 tpass-k8s-node1 tpass-k8s-node2
```

- 创建一个名为tpass的新用户

```shell
sudo useradd -m tpass -s /bin/bash
```

- 给tpass用户设置密码（请根据实际情况设定密码并记录）

```shell
sudo passwd tpass

# 账号/密码 
# tpass/tpassit168
```

- 给tpass用户sudo 权限

```shell
sudo adduser tpass sudo
```

- 切换一下tpass用户用于测试你创建的用户是否能登录

```shell
# 根据提示输入密码
su - tpass
```

- （可选）以下命令用于删除创建的用户，可以在创建错误的用户需要删除时使用

```shell
# sudo deluser --remove-home 用户名
```



### 解决规划2home和docker及ceph文件夹分别挂载在sdb磁盘的三个分区上

- 因为修改磁盘需要用到root用户，所以先给root用户设置密码（请根据实际情况设定密码并记录）

```shell
root@TPASS-K8S-master1:~# sudo passwd root

[sudo] password for tpass: tpassit168
Enter new UNIX password: tpassit168
Retype new UNIX password: tpassit168
passwd: password updated successfully

# 账号/密码 
# root/tpassit168
```

- 进入root用户

```shell
# 根据提示输入密码
su -
```

- 将当前挂载在sdb磁盘上的/home目录内的文件移动到其他目录中（请根据实际情况移动，我选择tmp）

```shell
# 进入/home目录
cd /home

# ll命令查看/home目录中的文件
root@TPASS-K8S-master1:~# ll

ubuntu
tpass
lost+found

# 移动文件到/tmp目录
mv ubuntu /tmp
mv tpass /tmp
mv lost+found /tmp

# 进入/tmp目录后,再ll查看是否存在刚刚移动过来的文件
cd /tmp
ll
```

- 查看一下/home目录是否被占用

```shell
fuser -m -u /home
who
```

- （可选）以下命令可以将占用/home目录的用户进行驱逐（不用驱逐root用户）

```shell
# pkill -u 用户名
# 假如驱逐占用/home目录的非root用户后，发现连接工具断开，请重新使用root用户登录即可，不必惊慌！
```

- 如果没有发现占用/home目录的用户，就可以直接卸载sdb磁盘上的/home目录

```shell
umount /home
```

- 查看磁盘情况

```shell
root@TPASS-K8S-master1:~# lsblk

loop0                               7:0    0 88.5M  1 loop /snap/core/7270
sda                                 8:0    0   16G  0 disk
├─sda1                              8:1    0    1M  0 part
└─sda2                              8:2    0   16G  0 part /
sdb                                 8:16   0  100G  0 disk
```

- 将/etc/fstab文件中的/home目录的UUID删除

```shell
root@TPASS-K8S-master1:~# vi /etc/fstab

# UUID=0471b702-bb0a-4fd5-8e69-5774a784b01e /home ext4 defaults 0 0
```

- 此时/home目录又指向到sda磁盘中，将存放在/tmp目录下的原/home目录内的文件移动到新的/home目录中（防止出现缺少文件后导致无法配置磁盘分区的现象）

```shell
cd /tmp
cp -rf ubuntu /home
cp -rf tpass /home
cp -rf lost+found /home

# 进入/home目录后,再ll查看是否存在刚刚移动过来的文件
cd /home
ll
```

- 创建磁盘分区

```shell
# 创建sdb磁盘分区
fdisk /dev/sdb
# 修改分区类型为GPT类型(刚开始就按)
Command:g
# 创建新的分区(一直回车)
Command:n
# 保存分区
Command:w

# 创建sdb中的pv
pvcreate /dev/sdb1

# 创建sdb中的vg
vgcreate volume-data /dev/sdb1

# 创建三个lv分区
lvcreate -L 1G -n lv-data-home volume-data
lvcreate -L 20G -n lv-data-docker volume-data
lvcreate -L 60G -n lv-data-nfs volume-data

# 查看创建好的lv
root@TPASS-K8S-master1:~# lvs

  LV             VG          Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lv-data-nfs   volume-data -wi-a----- 60.00g                                                    
  lv-data-docker volume-data -wi-a----- 20.00g                                                    
  lv-data-home   volume-data -wi-a-----  1.00g  
  
# 查看三个lv分区存放的路径lv:path
root@TPASS-K8S-master1:~# lvdisplay

/dev/volume-data/lv-data-home
/dev/volume-data/lv-data-docker
/dev/volume-data/lv-data-nfs
```

- 格式化三个lv分区

```shell
mkfs.ext4 /dev/volume-data/lv-data-home
mkfs.ext4 /dev/volume-data/lv-data-docker
mkfs.ext4 /dev/volume-data/lv-data-nfs
```

- 创建挂载的文件夹（因为/home已经存在，这里不再创建）

```shell
mkdir -p /mnt/nfs-data1
mkdir -p /var/lib/docker
```

- 创建三个lv分区的UUID

```shell
root@TPASS-K8S-master1:~# blkid /dev/volume-data/lv-data-home
root@TPASS-K8S-master1:~# blkid /dev/volume-data/lv-data-docker
root@TPASS-K8S-master1:~# blkid /dev/volume-data/lv-data-nfs

/dev/volume-data/lv-data-home: UUID="9917fd4a-f0f3-4eb7-ac8c-9199289c3af1" TYPE="ext4"
/dev/volume-data/lv-data-docker: UUID="b28f1bda-ccff-4620-a7bd-8d50c939373f" TYPE="ext4"
/dev/volume-data/lv-data-nfs: UUID="e28e00c2-6685-4db2-a820-5e185d2e49be" TYPE="ext4"
```

- 将创建好的UUID放入/etc/fstab文件下

```shell
UUID=092cf502-7d53-4371-8f15-53943a8aa5f1 /home ext4 defaults 0 0
UUID=84ec2cd3-8c93-4b9c-99cf-24a6021b092e /var/lib/docker ext4 defaults 0 0
UUID=d8e04e02-c943-42e3-ae82-2808598b3635 /mnt/nfs-data1 ext4 defaults 0 0
```

- 将创建好的文件夹挂载到sdb磁盘（挂载后的文件夹内部是空的）

```shell
# 挂载所有文件夹使用-a参数
mount -a

# 补充：挂载单个文件夹直接加文件夹名称
# mount /home
```

- 查看挂载的情况

```shell
root@TPASS-K8S-master1:~# lsblk

NAME                              MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                               7:0    0 88.5M  1 loop /snap/core/7270
sda                                 8:0    0   16G  0 disk
├─sda1                              8:1    0    1M  0 part
└─sda2                              8:2    0   16G  0 part /
sdb                                 8:16   0  100G  0 disk
└─sdb1                              8:17   0  100G  0 part
  ├─volume--data-lv--data--home   253:0    0    1G  0 lvm  /home
  ├─volume--data-lv--data--docker 253:1    0   20G  0 lvm  /var/lib/docker
  └─volume--data-lv--data--nfs   253:2    0   60G  0 lvm  /mnt/nfs-data1
sr0                                11:0    1 1024M  0 rom

```

- 虽然挂载成功了，但是需要将原先指向sda中的/home目录内的文件清除掉（会持续性生成文件占空间）卸载sdb磁盘中的/home目录，让其指向sda磁盘

```shell
umount /home
```

- 将移动到/home目录中的三个文件删除（/tmp目录内还有备份文件）

```shell
rm -rf lost+found
rm -rf tpass
rm -rf ubuntu
```

- 重新挂载/home目录，让其指向sdb磁盘

```shell
mount /home
```

- 再次还原/home目录内的三个文件

```shell
cd /tmp
cp -rf lost+found /home
cp -rf tpass /home
cp -rf ubuntu /home
```

### 解决规划3使用ansible软件控制其他电脑机器

- 因为ansible是通过ssh协议进行交互的，所有需要允许root用户通过ssh的登录

```shell
# 修改ssh配置文件
root@tpass-k8s-master1:~# vi /etc/ssh/sshd_config
# 在PermitRootLogin prohibit-password这行下面添加(安装完k8s集群和插件后需要删除掉)
PermitRootLogin yes

# 重启ssh服务
/etc/init.d/ssh restart
```



## 准备文件

|           文件名称            |                    ftp地址                    |              备注               |
| :---------------------------: | :-------------------------------------------: | :-----------------------------: |
|          ansible安装          |    ftp://192.168.10.9/软件/06 Kubernetes/     |  内含ansible和python2.7安装包   |
|            k8s安装            |    ftp://192.168.10.9/软件/06 Kubernetes/     | 内含k8s集群和监控系统需要的文件 |
|        k8s-plug.tar.gz        | ftp://192.168.10.9/软件/06 Kubernetes/k8s安装 |     内含插件系统相关文件包      |
| k8s-plug-docker-images.tar.gz | ftp://192.168.10.9/软件/06 Kubernetes/k8s安装 |   内含插件系统相关docker镜像    |
|  k8s-ansible-install.tar.gz   | ftp://192.168.10.9/软件/06 Kubernetes/k8s安装 |        K8s集群安装文件包        |
|   upload-docker-images.yml    | ftp://192.168.10.9/软件/06 Kubernetes/k8s安装 |     上传docker镜像操作文件      |



## k8s集群安装

>参考文档: https://github.com/easzlab/kubeasz/blob/master/docs/setup/offline_install.md

#### 安装前准备

- 所有操作请在root用户中进行
- 将k8s安装文件夹中的所有文件上传至命名为tpass-k8s-master1的机器中的/tmp目录下

- 在命名为tpass-k8s-master1的机器中安装ansible

```shell
# 确保ubuntu18.04_ansible.sh和ubuntu18.04_ansible.tar.gz两个文件在/tmp目录
# 运行shell脚本
sh ubuntu18.04_ansible.sh
```

- 在命名为tpass-k8s-node1和2的机器中安装python2.7

```shell
# 确保ubuntu18.04_python2.7.sh和ubuntu18.04_python2.7.tar.gz两个文件在/tmp目录
# 运行shell脚本
sh ubuntu18.04_python2.7.sh
```

- 将命名为tpass-k8s-master1中已经安装好的/etc/ansible文件夹删除

```shell
# 文件不要删除移动到其他目录备份
rm -rf /etc/ansible
```

- 将上传到命名为tpass-k8s-master1中的k8s-ansible-install.tar.gz文件解压到/etc目录中

```shell
tar zxvf /tmp/k8s2.0.2-ansible-install.tar.gz -C /etc
```

- 将/etc/ansible文件夹中含有 imagePullPolicy: Always 的yaml、yml、j2类型文件找出并修改成imagePullPolicy: IfNotPresent

```shell
root@tpass-k8s-master1:~# find /etc/ansible -name '*.yaml' | xargs -I {}  grep -l -i imagePullPolicy {} | xargs -I {} grep -l -i always {}

root@tpass-k8s-master1:~# find /etc/ansible -name '*.yml' | xargs -I {}  grep -l -i imagePullPolicy {} | xargs -I {} grep -l -i always {}

root@tpass-k8s-master1:~# find /etc/ansible -name '*.j2' | xargs -I {}  grep -l -i imagePullPolicy {} | xargs -I {} grep -l -i always {}

# 注解中有(无需修改)
/etc/ansible/manifests/mariadb-cluster/mariadb/values-production.yaml
/etc/ansible/manifests/mariadb-cluster/mariadb/values.yaml
/etc/ansible/manifests/mariadb-cluster/my-values.yaml
/etc/ansible/manifests/jenkins/values.yaml
# 配置中有(需要修改)
/etc/ansible/manifests/es-cluster/elasticsearch/templates/master-statefulset.yaml
/etc/ansible/manifests/es-cluster/elasticsearch/templates/data-statefulset.yaml
/etc/ansible/manifests/es-cluster/elasticsearch/templates/client-deployment.yaml

# 重启策略中有(无需修改)
/etc/ansible/roles/cilium/templates/cilium.yaml.j2
```



#### 开始安装（所有操作都在命名为tpass-k8s-master1机器中执行）

- 使用加密算法给集群配置ssh免密登录

```shell
# 默认使用第一种加密算法
ssh-keygen -t ed25519 -N '' -f ~/.ssh/id_ed25519

# 补充：第二种加密算法
ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa
```

- 将ssh密钥拷贝到集群主机中（请根据实际情况拷贝到对应机器中，按提示输入yes和虚拟机root用户的密码）

```shell
ssh-copy-id 86.20.19.30
ssh-copy-id 86.20.19.31
ssh-copy-id 86.20.19.32
```

- 将master主机中/etc/ansible/example/hosts.multi-node文件复制为ansible的hosts文件

```shell
cd /etc/ansible && cp example/hosts.multi-node hosts
```

- 将复制好的ansible的hosts文件修改成如下配置（只修改了etcd、kube-master、kube-node、chrony）

```shell
root@tpass-k8s-master1:~# vi hosts

# 'etcd' cluster should have odd member(s) (1,3,5,...)
# variable 'NODE_NAME' is the distinct name of a member in 'etcd' cluster
[etcd]
86.20.19.30 NODE_NAME=etcd1
86.20.19.31 NODE_NAME=etcd2
86.20.19.32 NODE_NAME=etcd3

# master node(s)
[kube-master]
86.20.19.30


# work node(s)
[kube-node]
86.20.19.31
86.20.19.32

# [optional] harbor server, a private docker registry
# 'NEW_INSTALL': 'yes' to install a harbor server; 'no' to integrate with existed one
[harbor]
#192.168.1.8 HARBOR_DOMAIN="harbor.yourdomain.com" NEW_INSTALL=no

# [optional] loadbalance for accessing k8s from outside
[ex-lb]
#192.168.1.6 LB_ROLE=backup EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443
#192.168.1.7 LB_ROLE=master EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443

# [optional] ntp server for the cluster
[chrony]
86.20.19.30

[all:vars]
# --------- Main Variables ---------------
# Cluster container-runtime supported: docker, containerd
CONTAINER_RUNTIME="docker"

# Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn
CLUSTER_NETWORK="flannel"

# K8S Service CIDR, not overlap with node(host) networking
SERVICE_CIDR="10.68.0.0/16"

# Cluster CIDR (Pod CIDR), not overlap with node(host) networking
CLUSTER_CIDR="172.20.0.0/16"

# NodePort Range
NODE_PORT_RANGE="20000-40000"

# Cluster DNS Domain
CLUSTER_DNS_DOMAIN="cluster.local."

# -------- Additional Variables (don't change the default value right now) ---
# Binaries Directory
bin_dir="/opt/kube/bin"

# CA and other components cert/key Directory
ca_dir="/etc/kubernetes/ssl"

# Deploy Directory (kubeasz workspace)
base_dir="/etc/ansible"
```

- 验证ansible 安装 , 正常能看到节点返回 SUCCESS （如果出现：discovered_interpreter_python": "/usr/bin/python，说明你的node节点没有安装python）

```shell
root@tpass-k8s-master1:~# ansible all -m ping

86.20.19.30 | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
86.20.19.31 | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
86.20.19.32 | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}

```

- 修改CA证书签名请求为以下配置（以下所有证书配置只修改了ST城市和L地区）


```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/deploy/templates/ca-csr.json.j2

{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "k8s",
      "OU": "System"
    }
  ],
  "ca": {
    "expiry": "{{ CA_EXPIRY }}"
  }
}
```

- 修改admin证书签名请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/deploy/templates/admin-csr.json.j2

{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
```

- 修改kube-proxy 证书请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/deploy/templates/kube-proxy-csr.json.j2

{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
```

-  修改read证书请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/deploy/templates/read-csr.json.j2

{
  "CN": "read",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "group:read",
      "OU": "System"
    }
  ]
}
```

- 修改etcd证书请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/etcd/templates/etcd-csr.json.j2

{
  "CN": "etcd",
  "hosts": [
{% for host in groups['etcd'] %}
    "{{ host }}",
{% endfor %}
    "127.0.0.1"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
```

- 修改kube-master中aggregator-proxy 证书签名请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/kube-master/templates/aggregator-proxy-csr.json.j2

{
  "CN": "aggregator",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
```

- 修改kube-master中kubernetes 证书签名请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/kube-master/templates/kubernetes-csr.json.j2

{
  "CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
{% if groups['ex-lb']|length > 0 %}
    "{{ hostvars[groups['ex-lb'][0]]['EX_APISERVER_VIP'] }}",
{% endif %}
    "{{ inventory_hostname }}",
    "{{ CLUSTER_KUBERNETES_SVC_IP }}",
{% for host in MASTER_CERT_HOSTS %}
    "{{ host }}",
{% endfor %}
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
```

- 修改kube-node中kubelet证书签名请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/kube-node/templates/kubelet-csr.json.j2

{
  "CN": "system:node:{{ inventory_hostname }}",
  "hosts": [
    "127.0.0.1",
    "{{ inventory_hostname }}"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "system:nodes",
      "OU": "System"
    }
  ]
}
```

- 修改calico网络证书签名请求为以下配置（使用90.setup.yml文件安装会装这个网络）


```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/calico/templates/calico-csr.json.j2

{
  "CN": "calico",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
```

- 修改harbor镜像仓库证书签名请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/harbor/templates/harbor-csr.json.j2

{
  "CN": "harbor",
  "hosts": [
    "127.0.0.1",
    "{{ inventory_hostname }}",
    "{{ HARBOR_DOMAIN }}"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
```

- 添加coredns域名（在health添加域名）

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/cluster-addon/templates/coredns.yaml.j2

apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        rewrite name drone.gzsw.gov.cn drone-drone.cicd.svc.cluster.local
        rewrite name gitea.gzsw.gov.cn gitea.cicd.svc.cluster.local
        rewrite name verdaccio.gzsw.gov.cn verdaccio-verdaccio.cicd.svc.cluster.local
        rewrite name registry.gzsw.gov.cn registry-service.cicd.svc.cluster.local
        ready
        kubernetes cluster.local. in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        reload
        loadbalance
    }

```

- 设置参数，将在线安装改为离线安装

```shell
sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: "offline"/g' /etc/ansible/roles/chrony/defaults/main.yml

sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: "offline"/g' /etc/ansible/roles/ex-lb/defaults/main.yml

sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: "offline"/g' /etc/ansible/roles/kube-node/defaults/main.yml

sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: "offline"/g' /etc/ansible/roles/prepare/defaults/main.yml

```

- 进入/etc/ansible文件夹中，分步安装集群 


```shell
ansible-playbook 01.prepare.yml
ansible-playbook 02.etcd.yml
ansible-playbook 03.docker.yml
ansible-playbook 04.kube-master.yml
ansible-playbook 05.kube-node.yml
ansible-playbook 06.network.yml
ansible-playbook 07.cluster-addon.yml

# 补充：一步安装
# ansible-playbook 90.setup.yml

```

- 查看集群安装情况


```shell
root@tpass-k8s-master1:~# kubectl get node

NAME            STATUS                     ROLES    AGE     VERSION
86.20.19.30   Ready,SchedulingDisabled   master   5m1s    v1.15.0
86.20.19.31   Ready                      node     3m34s   v1.15.0
86.20.19.32   Ready                      node     3m41s   v1.15.0

```

- 查看集群所有的命名空间

```shell
root@tpass-k8s-master1:~# kubectl get pod --all-namespaces
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   coredns-797455887b-5g987                      1/1     Running   0          2m21s
kube-system   coredns-797455887b-dgbbp                      1/1     Running   0          2m21s
kube-system   heapster-5f848f54bc-b8lw8                     1/1     Running   0          85s
kube-system   kube-flannel-ds-amd64-cghn8                   1/1     Running   0          2m57s
kube-system   kube-flannel-ds-amd64-qp9h9                   1/1     Running   0          2m57s
kube-system   kube-flannel-ds-amd64-t65cd                   1/1     Running   0          2m57s
kube-system   kubernetes-dashboard-5c7687cf8-h2x4k          1/1     Running   0          86s
kube-system   metrics-server-85c7b8c8c4-vlrzb               1/1     Running   0          2m8s
kube-system   traefik-ingress-controller-766dbfdddd-dql7w   1/1     Running   0          61s
```

- 修改master节点策略，允许容器在内运行。输入以下命令后再次查看集群情况发现master中SchedulingDisabled已经消失

```shell
kubectl patch node 86.20.19.30 -p '{"spec": {"unschedulable": false}}'
```

- 检查etcd集群状态，在任意 etcd 集群节点上执行如下命令 

```bash
root@tpass-k8s-master1:~# export NODE_IPS="86.20.19.30 86.20.19.31 86.20.19.32"
root@tpass-k8s-master1:~# for ip in ${NODE_IPS}; do
  ETCDCTL_API=3 etcdctl \
  --endpoints=https://${ip}:2379  \
  --cacert=/etc/kubernetes/ssl/ca.pem \
  --cert=/etc/etcd/ssl/etcd.pem \
  --key=/etc/etcd/ssl/etcd-key.pem \
  endpoint health; done
 
  
https://86.20.19.30:2379 is healthy: successfully committed proposal: took = 2.111512ms
https://86.20.19.31:2379 is healthy: successfully committed proposal: took = 4.530936ms
https://86.20.19.32:2379 is healthy: successfully committed proposal: took = 1.879708ms
```

- 开启 apiserver basic-auth(用户名/密码认证)，后面EFK插件会使用到

```shell
# 给文件写的权限
chmod +x /usr/bin/easzctl
# 开启认证并设置账号密码（请根据实际情况设定并记录）
easzctl basic-auth -s -u GZSW -p it168
```



#### 插件dashboard 仪表盘

**注：dashboard 在k8s集群安装好后就会自动创建好，所以只需要验证service服务后登录即可**

- 查看dashboard 运行状态

```shell
root@tpass-k8s-master1:~# kubectl get pod -n kube-system | grep dashboard

kubernetes-dashboard-5c7687cf8-rjmsx          1/1     Running   0          2m3s

```

- 查看dashboard service服务可以获得NodePort端口，用于登录


```shell
root@tpass-k8s-master1:~# kubectl get svc -n kube-system|grep dashboard

kubernetes-dashboard      NodePort    10.68.196.47    <none>        443:31845/TCP    2m3s 

```

##### dashboard 登录方式：令牌（token）

- 获取管理员身份的Token（该身份可以对集群进行管理），找到输出中 ‘token:’ 开头那一行，请保存好

```shell
root@tpass-k8s-master1:~# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWdnOHhkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwODUwZjgzOC00OTBlLTQwMzMtOTA4MS01NzU0ZjU5MWMyMmEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.Gtb5exfPer9zgxn9D-hEY9TlgRTIoLitHtXvJ6iQkIQzY0vEqzDUbJIHGDr_DrR9avYAcHdrROiHKfhJMTA2qrUviC8EOuIAnVvx-304tnAWH2m-Gr9ld2IqURioiUeSkW96q8ukhM9ZgLGfPfZ90UmwvRunOJH-GlikTKlzMqoru9b3rOfLDtd1hKPV4j1H3N38kBVm_4dGFIRBFE4k0ibSIRQiauacZmV8bZpECRGEFILZmGJliUgCJpRF0j3VnDwGtYCmDp4GSc0jhVI3MliDUtPtrMtIHXSo_vJRw5qreZQQPtiGWn-pX_vQSVdYFVM2JLGHFNoi2BfmyFJv0A

```

- 获取只读用户身份的token（该身份只能查看集群中的信息），找到输出中 ‘token:’ 开头那一行，请保存好

```shell
root@tpass-k8s-master1:~# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep read-user | awk '{print $1}')

token:
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtcmVhZC11c2VyLXRva2VuLXRyeGdmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRhc2hib2FyZC1yZWFkLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmOTA5YjMzZi1mZGM4LTQzZjktOGZjZC0wZDAyMjhhMDQ3ZWUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGFzaGJvYXJkLXJlYWQtdXNlciJ9.Gdcp2OpTlcANJw6Y9LCAwe4DmWSo78h87Sy19evJcGuDpIXdnNC_Y_KwhUWTVIaSgnt0kHcfdARdEjQh6tLZqSwziRuTmABJtYBxNVOogG3qvzd4OZXKeXUsjQEBgcyT8PUgH1fwljynprZ1i1l3AK6TAZ4YkIcZ5OtEHty61_koq5_7SdeG7Hhq65x-s6FaHDJfYgEuAZBTgZhu5ib2jkitMzYLs2uZ9m9zA2v2ch85x5qIwMchza9CcaYxpmj-BlEVVW-CRDvUl322WWw4jyjvaBViROWqkB_QhPd1zmNZODuXFjmeA9jqXUHrWFs1cTs6HMwwFie9_j-qdbPPcw
```

- 因为k8s证书不被认同，所以这里推荐使用火狐浏览器；协议为https，端口和上方查看service服务中看到的NodePort端口31845一致

```http
https://86.20.19.30:31845
```

- 在浏览器中选择-->高级-->接受风险并继续
- 看到Kubernetes 仪表板 ，选择令牌，然后将获取到的不同身份的token复制到输入框登录即可

##### dashboard 登录方式：配置文件（Kubeconfig）

由于配置文件方式登录的操作比较麻烦，这里不建议使用！感兴趣的小伙伴可以点击旁边链接阅读[dashboard使用配置文件方式打开](https://github.com/easzlab/kubeasz/blob/master/docs/guide/dashboard.md)



## haproxy安装

- 将haproxy的安装文件放入/tmp目录


```shell
# ubuntu18.04_haproxy.sh和ubuntu18.04_haproxy.tar.gz

sh ubuntu18.04_haproxy.sh
```

- 配置/etc/haproxy/haproxy.cfg


```
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

	# Default SSL material locations
	ca-base /etc/ssl/certs
	crt-base /etc/ssl/private

	# Default ciphers to use on SSL-enabled listening sockets.
	# For more information, see ciphers(1SSL). This list is from:
	#  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
	# An alternative list with additional directives can be obtained from
	#  https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy
	ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS
	ssl-default-bind-options no-sslv3

defaults
	log	global
	mode	http
	option	httplog
	option	dontlognull
        timeout connect 5000
        timeout client  50000
        timeout server  50000
	errorfile 400 /etc/haproxy/errors/400.http
	errorfile 403 /etc/haproxy/errors/403.http
	errorfile 408 /etc/haproxy/errors/408.http
	errorfile 500 /etc/haproxy/errors/500.http
	errorfile 502 /etc/haproxy/errors/502.http
	errorfile 503 /etc/haproxy/errors/503.http
	errorfile 504 /etc/haproxy/errors/504.http


frontend http_front
	bind *:80
	default_backend http_back

backend http_back
	server node1 86.20.18.100:80 

```



## NFS服务器安装

```shell
# 将ubuntu18.04_nfs-server.sh和ubuntu18.04_nfs-server.tar.gz放入/tmp目录
sh ubuntu18.04_nfs-server.sh
```

- 编辑`/etc/exports`文件添加需要共享目录，每个目录的设置独占一行，编写格式如下：

NFS共享目录路径 客户机IP或者名称(参数1,参数2,...,参数n)

```shell
/mnt/nfs-data1  10.68.0.0/16(rw,sync,insecure,no_subtree_check,no_root_squash) 172.20.0.0/16(rw,sync,insecure,no_subtree_check,no_root_squash) 86.20.19.30(rw,sync,insecure,no_subtree_check,no_root_squash) 86.20.19.31(rw,sync,insecure,no_subtree_check,no_root_squash) 86.20.19.32(rw,sync,insecure,no_subtree_check,no_root_squash)
```

- 配置完成后，您可以在终端提示符后运行以下命令来启动 NFS 服务器：

```shell
systemctl start nfs-kernel-server.service
```

- 查看可以访问的ip

```shell
root@test-k8s-node1:~# showmount -e 86.20.19.30
root@test-k8s-node1:~# showmount -e 86.20.19.31
root@test-k8s-node1:~# showmount -e 86.20.19.32

/mnt/nfs-data1 172.20.0.0/16,10.68.0.0/16,192.168.10.18,192.168.10.17,192.168.10.16
```



## k8s其他插件镜像上传

- 将上传到/tmp目录的插件压缩包解压

```shell
tar zxvf /tmp/k8s-plug.tar.gz -C /tmp
tar zxvf /tmp/k8s-plug-docker-images.tar.gz -C /tmp
```

- 使用ansible命令将插件压缩包分发并且解压到每个kube-node节点机器的/tmp目录，

```shell
ansible kube-node -m unarchive -a "src=/tmp/k8s-plug-docker-images.tar.gz dest=/tmp/ copy=yes"
```

- 在kube-master机器中运行插件镜像导入文件，可以将三台集群机器中的插件docker镜像包上传至docker软件内

```shell
ansible-playbook /tmp/upload-docker-images.yml
```

- 镜像导入完后删除不需要的文件

```shell
ansible etcd -m shell -a 'rm -rf /tmp/k8s-plug-docker-images'
rm /tmp/k8s-plug-docker-images.tar.gz
rm /tmp/k8s2.0.2-ansible-install.tar.gz
rm /tmp/upload-docker-images.yml
rm /tmp/k8s-plug.tar.gz
```



## 创建动态PV

-  编辑自定义配置文件：/etc/ansible/roles/cluster-storage/defaults/main.yml

```yaml
# 动态存储类型, 目前支持自建nfs和aliyun_nas
# 修改nfs_server 和 nfs_storage_class 和 nfs_provisioner_name可以启动多个pv
storage:
  # nfs server 参数
  nfs:
    enabled: "yes"
    server: "86.20.19.30"
    server_path: "/mnt/nfs-data1"
    storage_class: "nfs-1"
    provisioner_name: "nfs-provisioner-01"

  # aliyun_nas 参数
  aliyun_nas:
    enabled: "no"
    server: "xxxxxxxxxxx.cn-hangzhou.nas.aliyuncs.com"
    server_path: "/"
    storage_class: "class-aliyun-nas-01"
    controller_name: "aliyun-nas-controller-01"

```

- 创建 nfs provisioner 

```shell
ansible-playbook /etc/ansible/roles/cluster-storage/cluster-storage.yml
```

- 验证 nfs provisioner

```shell
root@tpass-k8s-master1:~# kubectl get pod --all-namespaces |grep nfs-prov
kube-system   nfs-provisioner-01-7d9ffffdf7-t4mvk           1/1     Running   0          3m46s
kube-system   nfs-provisioner-02-978d4949-s4b6c             1/1     Running   0          3m16s
kube-system   nfs-provisioner-03-56b8c5f4cc-48h7m           1/1     Running   0          111s

```



## 插件helm k8s包管理工具

- 修改helm证书签名请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/helm/templates/helm-csr.json.j2

{
  "CN": "{{ helm_cert_cn }}",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "k8s",
      "OU": "System"
    }
  ]
}


```

- 修改helm客户端tiller证书签名请求为以下配置

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/roles/helm/templates/tiller-csr.json.j2

{
  "CN": "{{ tiller_cert_cn }}",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangZhou",
      "L": "YX",
      "O": "k8s",
      "OU": "System"
    }
  ]
}


```



### 安装helm客户端

- 解压将上传好的helm-v2.14.1-linux-amd64.tar.gz压缩包

```shell
tar zxvf /tmp/k8s-plug/helm-v2.14.1-linux-amd64.tar.gz -C /tmp
```

- 将解压后得到的helm文件移动到所需的位置

```shell
cp /tmp/linux-amd64/helm /usr/local/bin/helm
```

- 查看版本（只能看到客户端的版本）

```shell
root@tpass-k8s-master1:~# helm version

Client: &version.Version{SemVer:"v2.14.1", GitCommit:"5270352a09c7e8b6e8c9593002a73535276507c0", GitTreeState:"clean"}
Error: could not find tiller

```



### 安装helm服务端tiller

- 创建本地repo仓库


```shell
mkdir -p /opt/helm-repo

```

- 启动helm repo server，在后台挂载一个本地仓库


```shell
nohup helm serve --address 127.0.0.1:8879 --repo-path /opt/helm-repo &

```

- 更改helm 配置文件,将/etc/ansible/roles/helm/defaults/main.yml中repo的地址改为 127.0.0.1:8879


```sh
cat <<EOF >/etc/ansible/roles/helm/defaults/main.yml
helm_namespace: kube-system 
helm_cert_cn: helm001
tiller_sa: tiller
tiller_cert_cn: tiller001
tiller_image: easzlab/tiller:v2.14.1
#repo_url: https://kubernetes-charts.storage.googleapis.com
history_max: 5
repo_url: http://127.0.0.1:8879
# 如果默认官方repo 网络访问不稳定可以使用如下的阿里云镜像repo
#repo_url: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
EOF

```

- 运行安装helm命令

```shell
ansible-playbook /etc/ansible/roles/helm/helm.yml

```

- 再次查看版本（可以看到完成版本）

```shell
root@tpass-k8s-master1:~# helm version

Client: &version.Version{SemVer:"v2.14.1", GitCommit:"5270352a09c7e8b6e8c9593002a73535276507c0", GitTreeState:"clean"}
Server: &version.Version{SemVer:"v2.14.1", GitCommit:"5270352a09c7e8b6e8c9593002a73535276507c0", GitTreeState:"clean"}

```



## helm安装Prometheus时序数据库

- 进入prometheus文件夹内


```shell
cd /etc/ansible/manifests/prometheus

```

- 安装 prometheus 软件包

```shell
helm install --tls \
        --name monitor \
        --namespace monitoring \
        -f prom-settings.yaml \
        -f prom-alertsmanager.yaml \
        -f prom-alertrules.yaml \
        prometheus

```

- 安装 grafana 软件包      

```shell
helm install --tls \
	--name grafana \
	--namespace monitoring \
	-f grafana-settings.yaml \
	-f grafana-dashboards.yaml \
	grafana

```

- 查看安装情况


```shell
root@tpass-k8s-master1:~# kubectl get pod,svc -n monitoring

NAME                                                         READY   STATUS    RESTARTS   AGE
pod/grafana-78747c76-cs92l                                   1/1     Running   0          2m46s
pod/monitor-prometheus-alertmanager-59cc5c4b6-6hfvc          2/2     Running   0          2m53s
pod/monitor-prometheus-kube-state-metrics-7556696ff7-pzw59   1/1     Running   0          2m53s
pod/monitor-prometheus-node-exporter-ttv4n                   1/1     Running   0          7s
pod/monitor-prometheus-node-exporter-w52mv                   1/1     Running   0          2m53s
pod/monitor-prometheus-node-exporter-xj9dl                   1/1     Running   0          2m53s
pod/monitor-prometheus-server-579954d46d-xwvx2               2/2     Running   0          2m53s

NAME                                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/grafana                                 NodePort    10.68.222.143   <none>        80:39002/TCP   2m46s
service/monitor-prometheus-alertmanager         NodePort    10.68.218.89    <none>        80:39001/TCP   2m53s
service/monitor-prometheus-kube-state-metrics   ClusterIP   None            <none>        80/TCP         2m53s
service/monitor-prometheus-node-exporter        ClusterIP   None            <none>        9100/TCP       2m53s
service/monitor-prometheus-server               NodePort    10.68.143.230   <none>        80:39000/TCP   2m53s



```

- 火狐浏览器访问prometheus的web界面：http://$NodeIP:39000

```http
http://86.20.19.30:39000

```

- 火狐浏览器访问alertmanager的web界面：http://$NodeIP:39001

```http
http://86.20.19.30:39001

```

- 火狐浏览器访问grafana的web界面：http://$NodeIP:39002 (默认用户密码 admin:admin，可在web界面修改)

```http
http://86.20.19.30:39002

```



## helm安装Ambassador网关

- 将上传好的ambassador-install.tar.gz包解压

```shell
tar zxvf /tmp/k8s-plug/ambassador-install.tar.gz -C /tmp

```

- 修改values.yaml

```yaml
---
# Default values for ambassador.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 3
daemonSet: false

# Enable autoscaling using HorizontalPodAutoscaler
# daemonSet: true, autoscaling will be disabled
autoscaling:
  enabled: false
  minReplicas: 2
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 60
    - type: Resource
      resource:
        name: memory
        targetAverageUtilization: 60

podDisruptionBudget: {}

# namespace:
  # name: default

# Additional container environment variable
env:
  {}
  # Exposing statistics via StatsD
  # STATSD_ENABLED: true
  # STATSD_HOST: statsd-sink
  # sets the minimum number of seconds between Envoy restarts
  # AMBASSADOR_RESTART_TIME: 15
  # sets the number of seconds that the Envoy will wait for open connections to drain on a restart
  # AMBASSADOR_DRAIN_TIME: 5
  # sets the number of seconds that Ambassador will wait for the old Envoy to clean up and exit on a restart
  # AMBASSADOR_SHUTDOWN_TIME: 10
  # labels Ambassador with an ID to allow for configuring multiple Ambassadors in a cluster
  # AMBASSADOR_ID: default

imagePullSecrets: []

securityContext:
  runAsUser: 8888

image:
  repository: quay.io/datawire/ambassador
  tag: 0.85.0
  pullPolicy: IfNotPresent

nameOverride: ""
fullnameOverride: ""
dnsPolicy: "ClusterFirst"
hostNetwork: false

service:
# type类型设置为LoadBalancer
  type: LoadBalancer

  # Note that target http ports need to match your ambassador configurations service_port
  # https://www.getambassador.io/reference/modules/#the-ambassador-module
  ports:
    - name: http
      port: 80
      targetPort: 8080
      # protocol: TCP
      # nodePort: 30080
    - name: https
      port: 443
      targetPort: 8443
      # protocol: TCP
      # nodePort: 30443
    # TCPMapping_Port
      # port: 2222
      # targetPort: 2222
      # protocol: TCP
      # nodePort: 30222

  annotations:
  #############################################################################
  ## Ambassador should be configured using CRD definition. If you want
  ## to use annotations, the following is an example of annotating the
  ## Ambassador service with global configuration manifest.
  ##
  ## See https://www.getambassador.io/reference/core/ambassador and
  ## https://www.getambassador.io/reference/core/tls for more info
  #############################################################################
  #
  #  getambassador.io/config: |
  #    ---
  #    apiVersion: ambassador/v1
  #    kind: TLSContext
  #    name: ambassador
  #    secret: ambassador-certs
  #    hosts: ["*"]
  #    ---
  #    apiVersion: ambassador/v1
  #    kind: Module
  #    name: ambassador
  #    config:
  #      admin_port: 8001
  #      diag_port: 8877
  #      diagnostics:
  #        enabled: true
  #      enable_grpc_http11_bridge: false
  #      enable_grpc_web: false
  #      enable_http10: false
  #      enable_ipv4: true
  #      enable_ipv6: false
  #      liveness_probe:
  #        enabled: true
  #      lua_scripts:
  #      readiness_probe:
  #        enabled: true
  #      server_name: envoy
  #      service_port: 8080
  #      use_proxy_proto: false
  #      use_remote_address: true
  #      xff_num_trusted_hops: 0
  #      x_forwarded_proto_redirect: false
  #      load_balancer:
  #        policy: round_robin
  #      circuit_breakers:
  #        max_connections: 2048
  #      retry_policy:
  #        retry_on: "5xx"
  #      cors:

adminService:
  create: true
  type: ClusterIP
  port: 8877
  # NodePort used if type is NodePort
  # nodePort: 38877
  annotations:
    {}

rbac:
  # Specifies whether RBAC resources should be created
  create: true
  podSecurityPolicies:
    {}

scope:
  # tells Ambassador to only use resources in the namespace or namespace set by namespace.name
  singleNamespace: false

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

initContainers: []

sidecarContainers: []

livenessProbe:
  initialDelaySeconds: 30
  periodSeconds: 3
  failureThreshold: 3

readinessProbe:
  initialDelaySeconds: 30
  periodSeconds: 3
  failureThreshold: 3


volumes: []

volumeMounts: []

podLabels:
  {}

podAnnotations:
  {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "9102"

deploymentAnnotations:
  {}
  # configmap.reloader.stakater.com/auto: "true"

resources:
  {}
  # If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 100m
  #  memory: 128Mi
  # requests:
  #  cpu: 100m
  #  memory: 128Mi

priorityClassName: ""

nodeSelector: {}

tolerations: []

affinity: {}

ambassadorConfig: ""

crds:
  enabled: true
  create: true
  keep: true

pro:
  enabled: false
  image:
    repository: quay.io/datawire/ambassador_pro
    tag: 0.10.0
    # The tag for the image of your custom Ambassador Pro container.
    # Leave unset if not using a custom build of Ambassador Pro.
    # See https://www.getambassador.io/docs/guides/filter-dev-guide/
    customBuildTag:
  # As of Ambassador Pro 0.6.0, both the RateLimitService and AuthService use the same port
  ports:
    auth: 8500
    ratelimit: 8500
  logLevel: info
  # A license key is required to use Ambassador Pro.
  # Get a license key by signing up for a free trial here: https://www.getambassador.io/pro/free-trial
  licenseKey:
    value: "{{INSERT LICENSE KEY HERE}}"
    # The license key will be stored and read from a Kubernetes secret named ambassador-pro-license-key
    # Set create: true for the first install and create: false for all subsequent installs
    secret:
      enabled: true
      create: true
  # Ambassador Pro environment variables can be found at https://www.getambassador.io/reference/pro/environment
  # For consistency, AMBASSADOR_ID is copied over from the Ambassador env above and will be ignored if set here.
  env:
    {}
  resources: {}
  # If you want to specify resources, uncomment the following
  # lines and remove the curly braces after 'resources:'.
  # These are placeholder values and must be tuned.
  #   limits:
  #     cpu: 100m
  #     memory: 256Mi
  #   requests:
  #     cpu: 50m
  #     memory: 128Mi

  authService:
    enabled: true
    # Set additional configuration options. See https://www.getambassador.io/reference/services/auth-service for more information
    optional_configurations:
      # include_body:
      #   max_bytes: 4096
      #   allow_partial: true
      # status_on_error:
      #   code: 403
      # failure_mode_allow: false
      # retry_policy:
      #   retry_on: "5xx"
      #   num_retries: 2
      # add_linkerd_headers: true
      # timeout_ms: 30000

  rateLimit:
    enabled: true
    redis:
      # Annotations for Ambassador Pro's redis instance.
      annotations:
        deployment:
          {}
        service:
          {}
      resources: {}
      # If you want to specify resources, uncomment the following
      # lines and remove the curly braces after 'resources:'.
      # These are placeholder values and must be tuned.
      #   limits:
      #     cpu: 100m
      #     memory: 256Mi
      #   requests:
      #     cpu: 50m
      #     memory: 128Mi
  devPortal:
    enabled: false

# DEPRECATED: Ambassador now exposes the /metrics endpoint in Envoy.
# DEPRECATED: See https://www.getambassador.io/user-guide/monitoring#deployment for more information on how to use the /metrics endpoint
#
# DEPRECATED: Enabling the prometheus exporter creates a sidecar and configures ambassador to use it
prometheusExporter:
  enabled: false
  repository: prom/statsd-exporter
  tag: v0.8.1
  pullPolicy: IfNotPresent
  resources: {}
  # If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  #   limits:
  #     cpu: 100m
  #     memory: 256Mi
  #   requests:
  #     cpu: 50m
  #     memory: 128Mi
  # You can configure the statsd exporter to modify the behavior of mappings and other features.
  # See documentation: https://github.com/prometheus/statsd_exporter/tree/v0.8.1#metric-mapping-and-configuration
  # Uncomment the following line if you wish to specify a custom configuration:
  # configuration: |
  #   ---
  #   mappings:
  #   - match: 'envoy.cluster.*.upstream_cx_connect_ms'
  #     name: "envoy_cluster_upstream_cx_connect_time"
  #     timer_type: 'histogram'
  #     labels:
  #       cluster_name: "$1"

```

- 进入/tmp目录执行安装命令

```shell
cd /tmp

helm install --name ambassador \
	ambassador
```

- 查看ambassador的service状态

```shell
root@tpass-k8s-master1:/tmp/ambassador# kubectl get svc
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)             ambassador         LoadBalancer   10.68.56.11    <pending>     80:25673/TCP,443:35163/TCP   7s
ambassador-admin   ClusterIP      10.68.24.247   <none>        8877/TCP                     7s
kubernetes         ClusterIP      10.68.0.1      <none>        443/TCP                      148m

```

- 浏览器访问地址

```http
http://86.20.19.30:38178/ambassador/v0/diag/

```

## helm安装metallb 软负载均衡器

- 将上传好的metallb-install.tar.gz包解压

```shell
tar zxvf /tmp/k8s-plug/metallb-install.tar.gz -C /tmp

```

- 修改values.yaml，将configInline:{}改为以下配置

```shell
configInline:
  address-pools:
  - name: default
    protocol: layer2
    addresses: 
    # 设置内部ip配置（最好是无人使用的网段）
    - 86.20.18.100-86.20.18.200

```

- 进入/tmp目录执行安装命令

```shell
cd /tmp

helm install --name metallb \
	metallb
```

- 查看metallb创建的状态

```shell
root@tpass-k8s-master1:~# kubectl get pod 

ambassador-6665495dc7-q9np8          1/1     Running   0          2m5s
ambassador-6665495dc7-vh8bq          1/1     Running   0          2m5s
ambassador-6665495dc7-wf9ft          1/1     Running   0          2m5s
metallb-controller-b7c58df4f-j57qd   1/1     Running   0          18s
metallb-speaker-5m774                1/1     Running   0          18s
metallb-speaker-ctfgb                1/1     Running   0          18s
metallb-speaker-pr2pv                1/1     Running   0          18s

```



## 插件EFK日志索引系统

- 修改EFK的挂载存储池文件es-statefulset.yaml（注意storageClassName与上方设置相同）

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/manifests/efk/es-dynamic-pv/es-statefulset.yaml

 volumeClaimTemplates:
  - metadata:
      name: elasticsearch-logging
    spec:
      accessModes: [ "ReadWriteMany" ]
      storageClassName: "nfs-1"
      resources:
        requests:
          storage: 4Gi

```

- 运行EFK安装文件


```shell
# 安装
kubectl apply -f /etc/ansible/manifests/efk/
kubectl apply -f /etc/ansible/manifests/efk/es-dynamic-pv/

# 在部署其他pv类型时可以删除
# kubectl delete -f /etc/ansible/manifests/efk/
# kubectl delete -f /etc/ansible/manifests/efk/es-dynamic-pv/
```

- 修改EFK系统的定时任务将时间30天改为180天

```shell
root@tpass-k8s-master1:~# vi /etc/ansible/manifests/efk/es-index-rotator/rotator.yaml

command:
-/bin/rotate.sh
- "180"

# 启动定时任务
kubectl apply -f /etc/ansible/manifests/efk/es-index-rotator/rotator.yaml
```

- 验证EFK安装


```shell
root@tpass-k8s-master1:~# kubectl get pods -n kube-system|grep -E 'elasticsearch|fluentd|kibana'

elasticsearch-logging-0                       1/1     Running   0          33s
elasticsearch-logging-1                       1/1     Running   0          23s
fluentd-es-v2.4.0-6tqbq                       1/1     Running   0          40s
fluentd-es-v2.4.0-qqgmp                       1/1     Running   0          40s
fluentd-es-v2.4.0-vhq4p                       1/1     Running   0          40s
kibana-logging-ffd866674-kcz98                1/1     Running   0          39s

```

- 获取kibana运行的地址

```shell
root@tpass-k8s-master1:~# kubectl cluster-info | grep Kibana

Kibana is running at https://86.20.19.30:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy

```

- 浏览器访问kibana地址(默认用户密码为之前认证的账号密码:GZSW ; it168)


```http
https://86.20.19.30:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy
```

- 创建kibana实例


1. 选择Management-->Index Patterns
2. 在index pattern输入框中输入默认的pattern：logstash-*	-->Next step
3. 在Time Filter field name下拉框中选择@timestamp -->Create index pattern
4. 日志索引已经完成，可以在界面中看到



## helm安装Gitea代码仓库

- 将上传好的gitea-install.tar.gz包解压

```
tar zxvf /tmp/k8s-plug/gitea-install.tar.gz -C /tmp
```

- 配置values.yaml文件

```yaml
# Default values for gitea.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  ## The official gitea image, change tag to use a different version.
  ## ref: https://hub.docker.com/r/gitea/gitea/tags/
  ##
  repository: gitea/gitea
  tag: 1.10.1
  pullPolicy: IfNotPresent

nameOverride: ""
fullnameOverride: ""

expose:
  # Set the way how to expose the service. Set the type as "ingress","clusterIP","LoadBalancer"
  # or "nodePort" and fill the information in the corresponding
  # section
  type: ingress
  #type: "LoadBalancer"
  tls:
    enabled: false
    # Fill the name of secret if you want to use your own TLS certificate
    # and private key. The secret must contain keys named tls.crt and
    # tls.key that contain the certificate and private key to use for TLS
    # The certificate and private key will be generated automatically if
    # it is not set
    secretName: ""
  ingress:
    enabled: true
    host: "gitea.gzsw.gov.cn"
    annotations:
      kubernetes.io/ingress.class: ambassador
      # kubernetes.io/ingress.class: nginx
      # nginx.ingress.kubernetes.io/ssl-redirect: "true"
      # nginx.ingress.kubernetes.io/proxy-body-size: "0"
      # kubernetes.io/tls-acme: "true"
  clusterIP:
    # The name of ClusterIP service
    # name: gitea
    ports:
      # The service port gitea listens on when serving with SSH
      ssh: 22
      # The service port gitea listens on when serving with HTTP
      http: 80
  nodePort:
    # The name of NodePort service
    # name: gitea
    ports:
      ssh:
        # The service port gitea listens on when serving with SSH
        port: 22
        # The node port gitea listens on when serving with SSH
        # nodePort: 30002
      http:
        # The service port gitea listens on when serving with HTTP
        port: 80
        # The node port gitea listens on when serving with HTTP
        # nodePort: 30003
  loadBalancer:
    annotations: []
    # The name of LoadBalancer service
    # name: gitea
    ports:
      # The service port gitea listens on when serving with SSH
      ssh: 22
      # The service port gitea listens on when serving with HTTP
      http: 80


## Enable persistence using Persistent Volume Claims
## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
##
persistence:
  enabled: true

  ## A manually managed Persistent Volume and Claim
  ## Requires persistence.enabled: true
  ## If defined, PVC must be created manually before volume will be bound
  # existingClaim:

  ## rabbitmq data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: "nfs-2"
  accessMode: ReadWriteOnce
  size: 2Gi

extraContainers: |

## additional volumes, e. g. for secrets used in an extraContainers.
##
extraVolumes: |

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 100m
  #  memory: 128Mi
  # requests:
  #  cpu: 100m
  #  memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

```

- 安装gitea

```shell
cd /tmp

helm install --name gitea \
  --namespace cicd\
  --set resources.limits.memory=512Mi \
  gitea
```

- 验证gitea

```shell
root@TPASS-K8S-MASTER1:~# kubectl get svc -n cicd 

gitea                 ClusterIP   10.68.120.21    <none>        22/TCP,80/TCP   2d6h
```

- 浏览器访问（访问的电脑需要添加"86.20.19.148 gitea.gzsw.gov.cn"域名）

```http
http://gitea.gzsw.gov.cn/
```

### 进入gitea初始配置

```
数据库设置
数据类型为：SQLite3

服务器和第三方服务设置
启用本地模式
禁用Gravatar头像
禁止用户自助注册
启用页面访限制
默认情况下允许创建组织
默认情况下启用时间跟踪

管理员账号设置
管理员用户名：admin
管理员密码：it168
电子邮件地址：wangjing@hongmeng-info.com

执行安装
```

### 进入gitea首页

#### 注册用户

- 选择页面右上角注册

- 根据实际情况注册账号

- 使用注册好的用户登录

  

#### 创建仓库

- 选择用户首页中仓库列表右侧的"+"号创建仓库
- 根据实际情况配置仓库并创建



#### 创建组织

- 选择用户首页中组织选项
- 点击我的组织右侧的"+"号创建组织
- 根据实际情况配置组织并创建



#### 创建新的OAuth2应用程序（安装drone时需要）

- 点击用户首页右上角的头像
- 选择设置-->应用
- 找到最下面的“创建新的OAuth2应用程序”

```
# 应用名称：
drone

# 重定向URI（下一步安装drone会配置）
http://drone.gzsw.gov.cn/login

# 创建应用
```

- 编辑 OAuth2 应用程序 

```
# 客户端ID（自动生成，请记录好，配置drone需要使用）
6f3f898e-f68d-4288-a9d3-49a89d0620e4

# 客户端密钥（自动生成，请记录好，配置drone需要使用）
k1zU2RFKZon-9XD_ADDxR2Ir0MxhzkP8tb05NrOGkp0=

# 应用名称
drone

# 重定向URI
http://drone.gzsw.gov.cn/login

# 保存（客户端密钥会遮掩）
```



## helm安装drone CI工具

- 将上传好的drone-install.tar.gz包解压

```
tar zxvf /tmp/k8s-plug/drone-install.tar.gz -C /tmp
```

- 配置values.yaml文件

```yaml
images:
  ## The official drone (server) image, change tag to use a different version.
  ## ref: https://hub.docker.com/r/drone/drone/tags/
  ##
  server:
    repository: "drone/drone"
    tag: 1.6.2
    pullPolicy: IfNotPresent

  ## The official drone (agent) image, change tag to use a different version.
  ## ref: https://hub.docker.com/r/drone/agent/tags/
  ##
  agent:
    repository: "drone/agent"
    tag: 1.6.2
    pullPolicy: IfNotPresent

  ## The official docker (dind) image, change tag to use a different version.
  ## ref: https://hub.docker.com/r/library/docker/tags/
  ##
  dind:
    repository: "docker.io/library/docker"
    tag: 19.03.4-dind
    pullPolicy: IfNotPresent

service:
  httpPort: 80

  ## If service.type is not set to NodePort, the following statement
  ## will be ignored.
  ##
  # nodePort: 34904

  ## Service type can be set to ClusterIP, NodePort or LoadBalancer.
  ##
  type: ClusterIP

  ## Specify a load balancer IP address to use if your provider supports it.
  # loadBalancerIP:

  ## Drone Service annotations
  ##
  # annotations:
  #   service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
  #   service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:xx-xxxx-x:xxxxxxxxxxx:certificate/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx
  #   external-dns.alpha.kubernetes.io/hostname: drone.domain.tld.

  ## set to true if you want to expose drone's GRPC via the service (for external access)
  exposeGRPC: false

ingress:
  ## If true, Drone Ingress will be created.
  ##
  enabled: true
  hosts: 
  - drone.gzsw.gov.cn
  annotations:
    kubernetes.io/ingress.class: ambassador

  ## Drone Ingress annotations
  ##
  # annotations:
  #   kubernetes.io/ingress.class: nginx
  #   kubernetes.io/tls-acme: 'true'

  ## Drone hostnames must be provided if Ingress is enabled
  ##
  # hosts:
  #   - drone.domain.io

  ## Drone Ingress TLS configuration secrets
  ## Must be manually created in the namespace
  ##
  # tls:
  #   - secretName: drone-tls
  #     hosts:
  #       - drone.domain.io

sourceControl:
  ## your source control provider: github,gitlab,gitea,gogs,bitbucketCloud,bitbucketServer
  provider: gitea
  ## secret containing your source control provider secrets, keys provided below.
  ## if left blank will assume a secret based on the release name of the chart.
  secret:
  ## Fill in the correct values for your chosen source control provider
  ## Any key in this list with the suffix `Key` will be fetched from the
  ## secret named above, if not provided the secret it will be created as
  ## `<fullName>-source-control` using for the key "ClientSecretKey" and
  # "clientSecretValue" for the value. Be awere to not leak shis file with your password
  #github:
   # clientID:
   # clientSecretKey: clientSecret
   # clientSecretValue:
   # server: https://github.com
  #gitlab:
   # clientID:
   # clientSecretKey: clientSecret
   # clientSecretValue:
   # server:
   # 根据gitea中应用情况配置
  gitea:
    clientID: 0a6aa161-5f29-4a78-979e-a4f549dedb56
    clientSecretKey: clientSecret
    clientSecretValue: 3SIeKj528t-woIh_MXPHu25uc6jUxFD-7gDz5L7H4nk=
    server: http://gitea.gzsw.gov.cn/
  #gogs:
   # server:
  #bitbucketCloud:
   # clientID:
   # clientSecretKey: clientSecret
   # clientSecretValue:
  #bitbucketServer:
   # server:
   # consumerKey: consumerKey
   # privateKey: privateKey
   # username:
   # passwordKey: password

server:
  ## If not set, it will be autofilled with the cluster host.
  ## Host shoud be just the hostname.
  ##
  # host: "drone.domain.io"
  host: "drone.gzsw.gov.cn"

  ## protocol should be http or https
  protocol: http

  ## Initial admin user
  ## Leaving this blank may make it impossible to log into drone.
  ## Set to a valid oauth user from your git/oauth server
  ## For more complex user creation you can use env variables below instead.
  # 根据实际情况配置
  adminUser: hongmeng

  ## Configures Drone to authenticate when cloning public repositories. This is only required
  ## when your source code management system (e.g. GitHub Enterprise) has private mode enabled.
  alwaysAuth: true

  ## Configures drone to use kubernetes to run pipelines rather than agents, if enabled
  ## will not deploy any agents.
  kubernetes:
    ## set to true if you want drone to use kubernetes to run pipelines
    enabled: true
    ## you can run pipeline jobs in another namespace, if you choose to do this
    ## you'll need to create that namespace manually.
    # namespace:

    ## alternative service account to create to create drone pipelines. this account
    ## will be given cluster-admin rights.
    ## if not set the rights will be given to the default drone service account name.
    # pipelineServiceAccount:

  ## Drone server configuration.
  ## Values in here get injected as environment variables.
  ## You can set up remote database servers etc using environment
  ## variables.
  ## ref: https://docs.drone.io/reference/server/
  ##
  env:
    DRONE_LOGS_DEBUG: "false"
    DRONE_DATABASE_DRIVER: "sqlite3"
    DRONE_DATABASE_DATASOURCE: "/var/lib/drone/drone.sqlite"

  ## Secret environment variables are configured in `server.envSecrets`.
  ## Each item in `server.envSecrets` references a Kubernetes Secret.
  ## These Secrets should be created before they are referenced.
  ##
  # envSecrets:
  #   # The name of a Kubernetes Secret
  #   drone-server-secrets:
  #     # A list of Secret keys to include as environment variables
  #     - DRONE_GITHUB_SECRET

  ## Additional server annotations.
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  annotations: {}

  ## CPU and memory limits for drone server
  ##
  resources: {}
  #  requests:
  #    memory: 32Mi
  #    cpu: 40m
  #  limits:
  #    memory: 2Gi
  #    cpu: 1

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  ## Pod scheduling preferences.
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  ## Node labels for pod assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection
  ##
  nodeSelector: {}

  ## additional siecar containers, e. g. for a database proxy, such as Google's cloudsql-proxy.
  ## ex: https://github.com/kubernetes/charts/tree/master/stable/keycloak
  ##
  extraContainers: |

  ## additional volumes, e. g. for secrets used in an extraContainers.
  ##
  extraVolumes: |

agent:
  ## Drone agent configuration.
  ## Values in here get injected as environment variables.
  ## ref: https://docs.drone.io/reference/agent/
  ##
  env:
    DRONE_LOGS_DEBUG: "false"

  ## Number of drone agent replicas
  replicas: 1

  ## Additional agent annotations.
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  annotations: {}

  ## CPU and memory limits for drone agent
  ##
  resources: {}
  #  requests:
  #    memory: 32Mi
  #    cpu: 40m
  #  limits:
  #    memory: 2Gi
  #    cpu: 1

  ## Liveness and readiness probe values
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## drone agent does not currently have a health endpoint to check against.
  livenessProbe: {}
  readinessProbe: {}

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  ## Pod scheduling preferences.
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  ## Node labels for pod assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection
  ##
  nodeSelector: {}

dind:
  ## Enable or disable DinD
  ## If disabled, the drone agent will spawn docker containers on the host. Pay
  ## attention to the fact that we can't enforce resource constraints in that case.
  ##
  enabled: true

  ## Values in here get injected as environment variables to DinD.
  ## ref: http://readme.drone.io/admin/installation-reference
  ##
  #  env:
  #    DRONE_DEBUG: "false"

  ## Allowing custom command and args to DinD
  ## ref: https://discourse.drone.io/t/docker-mtu-problem/1207
  ##
  #  command: '["/bin/sh"]'
  #  args: '["-c", "dockerd --host=unix:///var/run/docker.sock --host=tcp://127.0.0.1:2375 --mtu=1350"]'

  ## Docker storage driver.
  ## Your DinD instance should be using the same driver as your host.
  ## ref: https://docs.docker.com/engine/userguide/storagedriver/selectadriver/
  ##
  driver: overlay2

  ## CPU and memory limits for dind
  ##
  resources: {}
  #  requests:
  #    memory: 32Mi
  #    cpu: 40m
  #  limits:
  #    memory: 2Gi
  #    cpu: 1

## Enable scraping of the /metrics endpoint for Prometheus
metrics:
  prometheus:
    enabled: false

## Enable persistence using Persistent Volume Claims
## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
##
persistence:
  enabled: true

  ## A manually managed Persistent Volume and Claim
  ## Requires persistence.enabled: true
  ## If defined, PVC must be created manually before volume will be bound
  # existingClaim:

  ## rabbitmq data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: "nfs-3"
  accessMode: ReadWriteOnce
  size: 1Gi

## Uncomment this if you want to set a specific shared secret between
## the agents and servers, otherwise this will be auto-generated.
##
# sharedSecret: supersecret

rbac:
  ## Specifies whether RBAC resources should be created
  create: true
  ## RBAC api version (v1, v1beta1, or v1alpha1)
  apiVersion: v1

serviceAccount:
  ## Specifies whether a ServiceAccount should be created
  create: true
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  name:

```

- 安装drone

```shell
cd /tmp

helm install --name drone  \
  --namespace cicd  \
  drone
```

- 验证drone安装

```shell
root@tpass-k8s-master1:~# kubectl get svc -n gitea
NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)                     AGE
drone-drone           ClusterIP   10.68.164.92    <none>        80/TCP          2d6h
drone-drone-grpc      ClusterIP   10.68.237.142   <none>        9000/TCP        2d6h

```

- 浏览器访问（访问的电脑需要添加"86.20.19.148 drone.gzsw.gov.cn"域名）

```http
http://drone.gzsw.gov.cn/
```

- 进入drone界面需要gitea管理员授权
- 授权成功后可以看到对应的gitea仓库



## helm安装verdaccio

- 将上传好的verdaccio-install.tar.gz包解压

```
tar zxvf /tmp/k8s-plug/verdaccio-install.tar.gz -C /tmp
```

- 配置values.yaml文件


```yaml
image:
  repository: verdaccio/verdaccio
  tag: 3.11.6
  pullPolicy: IfNotPresent

service:
  annotations: {}
  clusterIP: ""

  ## List of IP addresses at which the service is available
  ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  ##
  externalIPs: []

  loadBalancerIP: ""
  loadBalancerSourceRanges: []
  port: 4873
  type: ClusterIP
  # type: LoadBalancer
  # nodePort: 31873

## Node labels for pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

## Tolerations for nodes
tolerations: []

podAnnotations: {}
replicaCount: 1

resources: {}
  # limits:
  #  cpu: 100m
  #  memory: 512Mi
  # requests:
  #  cpu: 100m
  #  memory: 512Mi

ingress:
  enabled: true
  hosts:
  - verdaccio.gzsw.gov.cn
  annotations:
    kubernetes.io/ingress.class: ambassador
#   - npm.blah.com
# annotations:
#   kubernetes.io/ingress.class: nginx
# tls:
#   - secretName: secret
#     hosts:
#       - npm.blah.com

configMap: |
  # This is the config file used for the docker images.
  # It allows all users to do anything, so don't use it on production systems.
  #
  # Do not configure host and port under `listen` in this file
  # as it will be ignored when using docker.
  # see https://github.com/verdaccio/verdaccio/blob/master/docs/docker.md#docker-and-custom-port-configuration
  #
  # Look here for more config file examples:
  # https://github.com/verdaccio/verdaccio/tree/master/conf
  #

  # path to a directory with all packages
  storage: /verdaccio/storage/data

  web:
    # WebUI is enabled as default, if you want disable it, just uncomment this line
    #enable: false
    title: Verdaccio

  auth:
    htpasswd:
      file: /verdaccio/storage/htpasswd
      # Maximum amount of users allowed to register, defaults to "+infinity".
      # You can set this to -1 to disable registration.
      max_users: 1

  # a list of other known repositories we can talk to
  uplinks:
    npmjs:
      url: https://registry.npmjs.org/

  packages:
    '@*/*':
      # scoped packages
      access: $all
      publish: $authenticated
      proxy: npmjs

    '**':
      # allow all users (including non-authenticated users) to read and
      # publish all packages
      #
      # you can specify usernames/groupnames (depending on your auth plugin)
      # and three keywords: "$all", "$anonymous", "$authenticated"
      access: $all

      # allow all known users to publish packages
      # (anyone can register by default, remember?)
      publish: $authenticated

      # if package is not available locally, proxy requests to 'npmjs' registry
      proxy: npmjs

  # To use `npm audit` uncomment the following section
  middlewares:
    audit:
      enabled: true

  # log settings
  logs:
    - {type: stdout, format: pretty, level: http}
    #- {type: file, path: verdaccio.log, level: info}

persistence:
  enabled: true
  ## A manually managed Persistent Volume and Claim
  ## Requires Persistence.Enabled: true
  ## If defined, PVC must be created manually before volume will be bound
  # existingClaim:

  ## Verdaccio data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: "nfs-2"

  accessMode: ReadWriteOnce
  size: 2Gi
  volumes:
#  - name: nothing
#    emptyDir: {}
  mounts:
#  - mountPath: /var/nothing
#    name: nothing
#    readOnly: true

securityContext:
  enabled: true
  runAsUser: 100
  fsGroup: 101

```

- 安装verdaccio

```shell
cd /tmp

 helm install --name verdaccio \
   --namespace cicd \
   verdaccio
```

- 验证verdaccio

```shell
root@tpass-k8s-master1:~# kubectl get pod -n cicd
NAME                                   READY   STATUS    RESTARTS   AGE
verdaccio-verdaccio   ClusterIP   10.68.150.116   <none>        4873/TCP        32h

```

- 浏览器访问（访问的电脑需要添加"86.20.19.148 verdaccio.gzsw.gov.cn"域名）


```http
http://verdaccio.gzsw.gov.cn/
```

- 使用一台内网有npm环境的机器访问地址并创建用户

```
# 访问地址
npm adduser --registry http://verdaccio.gzsw.gov.cn

# 根据提示输入用户、密码和邮箱地址
hongmeng
it168
hongmeng@info.com

# 再次使用浏览器访问地址即可登录
```



## 设置私服仓库registry

- k8s启动registry

```yaml
# PersistentVolumeClaim
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: registry-pv-claim
  namespace: cicd
  labels:
    app: k8s-registry
spec:
  storageClassName: nfs-3
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

# Deployment
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-registry
  namespace: cicd
  labels:
    app: k8s-registry
    kubernetes.io/cluster-service: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: k8s-registry
  template:
    metadata:
      labels:
        app: k8s-registry
        kubernetes.io/cluster-service: "true"
    spec:
      #imagePullSecrets:
        #- name: myregistrykey
      containers:
      - name: registry
        image: registry:2
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        env:
        # Configuration reference: https://docs.docker.com/registry/configuration/
        - name: REGISTRY_AUTH
          value: htpasswd
        - name: REGISTRY_AUTH_HTPASSWD_REALM
          value: Registry Realm
        - name: REGISTRY_AUTH_HTPASSWD_PATH
          value: /auth/htpasswd
        volumeMounts:
        - name: image-store
          mountPath: /var/lib/registry
        ports:
        - containerPort: 5000
          name: registry
      volumes:
      - name: image-store
        persistentVolumeClaim:
          claimName: registry-pv-claim
          readOnly: false

# Service
---
apiVersion: v1
kind: Service
metadata:
  name: registry-service
  namespace: cicd
spec: 
  selector:
    app: k8s-registry
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000

# Ingress
--- 
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: registry-service
  namespace: cicd
  labels:
    app: k8s-registry 
  annotations: 
    kubernetes.io/ingress.class: ambassador
spec:
  rules:
    - host: registry.gzsw.gov.cn
      http:
       paths:
       - path: /
         backend:
           serviceName: registry-service
           servicePort: 80

```

- 查看registry状态

```shell
root@tpass-k8s-master1:~# kubectl get pod -n cicd 
NAME                                   READY   STATUS      RESTARTS   AGE
k8s-registry-6f798c8bf8-tkwhs          1/1     Running     0          11m
```

- 修改 /etc/docker/daemon.json，添加以下内容

```
 "insecure-registries":["registry.gzsw.gov.cn"]
```

- 修改宿主机/etc/hosts文件，添加以下内容


```shell
86.20.18.100 registry.gzsw.gov.cn
```

- 重启docker

```
systemctl restart docker
```

- 创建账号密码

```shell
 kubectl -n cicd exec -it k8s-registry-6f798c8bf8-tkwhs -- sh -c 'htpasswd -Bbn hongmeng it168 > /auth/htpasswd'
```

- 浏览器访问（访问的电脑需要添加"86.20.19.148 registry.gzsw.gov.cn"域名）

```http
# 按提示输入账号密码
http://registry.gzsw.gov.cn/v2/_catalog
```

### 将现有的镜像上传到私服上

- 将要上传镜像标记为私服镜像（nginx:latest为例）

```shell
docker tag nginx:latest registry.gzsw.gov.cn/tpass-api/nginx:latest
```

- 在shell终端登录账号密码

```shell
root@test-k8s-master1:~# docker login -u hongmeng -p it168 registry.gzsw.gov.cn

Login Succeeded

# 登出registry账号
# root@test-k8s-master1:~# docker logout registry.gzsw.gov.cn
```

- 将标记了的镜像push上私服中（push完后刷新浏览器查看）

```shell
root@test-k8s-master1:~# docker push registry.gzsw.gov.cn/tpass-api/nginx:latest

The push refers to repository [registry.gzsw.gov.cn/nginx]
75248c0d5438: Pushed 
49434cc20e95: Pushed 
556c5fb0d91b: Pushed 
latest: digest: sha256:36b77d8bb27ffca25c7f6f53cadd059aca2747d46fb6ef34064e31727325784e size: 948
```



## 配置附录

### 在访问的windows电脑上配置hosts

```
86.20.19.30 gitea.gzsw.gov.cn
86.20.19.30 drone.gzsw.gov.cn
86.20.19.30 verdaccio.gzsw.gov.cn
86.20.19.30 registry.gzsw.gov.cn
```

### 在master1宿主机上配置hosts

```
86.20.18.100 registry.gzsw.gov.cn
```



## 问题附录

### 安装后使用tab键遇到的问题

```shell
_get_comp_words_by_ref: command not found
```

- 解决：

```shell
source /etc/bash_completion
source <(kubectl completion bash)
```

### helm卸载安装的插件

```shell
# 查看helm安装的应用
helm list

# 删除应用
helm delete 应用名称 --purge
```



### 卸载stable/ambassador时遇到的问题

```
Error: customresourcedefinitions.apiextensions.k8s.io "consulresolvers.getambassador.io" already exists

```

- 解决：执行命令删除相关的crds即可:

```shell
# 查看已有的crds
root@tpass-k8s-master1:~# kubectl get crds|grep ambassador

authservices.getambassador.io                  2019-12-11T09:45:35Z
consulresolvers.getambassador.io               2019-12-11T09:45:37Z
filterpolicies.getambassador.io                2019-12-11T09:45:38Z
filters.getambassador.io                       2019-12-11T09:45:39Z
kubernetesendpointresolvers.getambassador.io   2019-12-11T09:45:40Z
kubernetesserviceresolvers.getambassador.io    2019-12-11T09:45:41Z
mappings.getambassador.io                      2019-12-11T09:45:42Z
modules.getambassador.io                       2019-12-11T09:45:43Z
ratelimits.getambassador.io                    2019-12-11T09:45:44Z
ratelimitservices.getambassador.io             2019-12-11T09:45:45Z
tcpmappings.getambassador.io                   2019-12-11T09:45:46Z
tlscontexts.getambassador.io                   2019-12-11T09:45:47Z
tracingservices.getambassador.io               2019-12-11T09:45:48Z


# 删除相关的crds
kubectl get crds|grep ambassador | awk '{ print $1; }' | xargs -I {}  kubectl delete crds {}

```

### 出现创建pod无法挂载情况pending：/etc/exports文件格式不正确

```shell
# ip后面不要有空格，与()紧密相连
/mnt/nfs       *(rw,sync,insecure,no_subtree_check,no_root_squash)

# 重启动nfs服务
systemctl restart nfs-kernel-server.service
```



## 广州市税务局目前安装情况

测试环境三台机器

- 每台机器都设置了自己的主机名（tpass-k8s-master1，tpass-k8s-node1，tpass-k8s-node2）

- 每台机器添加了一位tpass用户
- 每台机器允许root用户通过ssh的登录

- 硬件sdb100G磁盘空间使用情况：

  1G  赋予/home挂载

  60G 赋予/mnt/nfs-data1挂载

  20G 赋予  /var/lib/docker挂载

  剩余19G 待使用 

- 在命名为tpass-k8s-master1机器安装了ansible2.5

- 在命名为tpass-k8s-node1和2机器安装了python2.7

- k8s集群已经安装情况：

  k8s集群+自身dashboard插件

  helm插件

  prometheus插件

  alertmanager插件

  grafana插件

  ambassador插件

  metallb插件

  



生产环境三台机器

- 每台机器都设置了自己的主机名（tpass-k8s-master1，tpass-k8s-node1，tpass-k8s-node2）

- 每台机器添加了一位tpass用户
- 每台机器允许root用户通过ssh的登录

- 硬件sdb100G磁盘空间使用情况：

  1G  赋予/home挂载

  60G 赋予/mnt/nfs-data1挂载

  20G 赋予  /var/lib/docker挂载

  剩余19G 待使用 

- 在命名为tpass-k8s-master1机器安装了ansible2.5

- 在命名为tpass-k8s-node1和2机器安装了python2.7

- k8s集群已经安装情况：

  k8s集群+自身dashboard插件

  helm插件

  prometheus插件

  alertmanager插件

  grafana插件

  ambassador插件

  metallb插件